{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ModelV1 : Changed final few layers, and few changes..."
      ],
      "metadata": {
        "id": "CjC8z7UsJAcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UFSv2P1aI_n5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = r\"E:\\THIRD YEAR\\Datasets\\deepglobe\\smallSizeTrain\"\n",
        "VALID_DIR = r\"E:\\THIRD YEAR\\Datasets\\deepglobe\\smallSizeTrain\"\n",
        "TEST_DIR = r\"E:\\THIRD YEAR\\Datasets\\deepglobe\\smallSizeTest\"\n",
        "# COLOR_CODES = r\"E:\\THIRD YEAR\\Datasets\\deepglobe\\class_dict.csv\"\n",
        "COLOR_CODES = \"E:\\\\THIRD YEAR\\\\Datasets\\\\deepglobe\\\\class_dict.csv\"\n",
        "\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Augmentation\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "print(os.path.exists(COLOR_CODES))\n",
        "print(os.path.exists(TRAIN_DIR))\n"
      ],
      "metadata": {
        "id": "Tg_oWIsvJJYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777a033a-ea6b-44c2-9624-eb9daa5965a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(COLOR_CODES)\n",
        "label_map = {}\n",
        "for index, row in df.iterrows():\n",
        "    label_map[index] = [row[\"r\"],row[\"g\"],row[\"b\"]]\n",
        "label_map\n",
        "\n",
        "# label_map = {\n",
        "#     0: [0, 255, 255],  # class1\n",
        "#     1: [255, 255, 0],  # class2\n",
        "#     2: [255, 0, 255],  # class3\n",
        "#     3: [0, 255, 0],    # class4\n",
        "#     4: [0, 0, 255],    # class5\n",
        "#     5: [255, 255, 255],# class6\n",
        "#     6: [0, 0, 0]       # class7\n",
        "# }\n",
        "# label_map"
      ],
      "metadata": {
        "id": "dA-Kr0_xJeRe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "31a7292f-b111-49ed-b046-b5c3ef08b013"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'E:\\\\THIRD YEAR\\\\Datasets\\\\deepglobe\\\\class_dict.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f169d593c4f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLOR_CODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\THIRD YEAR\\\\Datasets\\\\deepglobe\\\\class_dict.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Segmentation_Dataset(Dataset):\n",
        "    def __init__(self, image_dir, label_map, image_transform=None, mask_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.label_map = label_map\n",
        "\n",
        "        # ðŸ› ï¸ FIX: Use `natsorted()` to ensure correct file mapping\n",
        "        self.images_name = natsorted([f for f in os.listdir(self.image_dir) if f.endswith('_sat.jpg')])\n",
        "        self.targets_name = natsorted([f for f in os.listdir(self.image_dir) if f.endswith('_mask.png')])\n",
        "\n",
        "        print(f\"Number of images: {len(self.images_name)}\")\n",
        "        print(f\"Number of masks: {len(self.targets_name)}\")\n",
        "\n",
        "        assert len(self.images_name) == len(self.targets_name), \"Mismatch in images and masks count!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_name)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.images_name):\n",
        "            raise IndexError(f\"Index {idx} out of range for dataset length {len(self.images_name)}.\")\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, self.images_name[idx])\n",
        "        image = cv2.imread(image_path)  # Load as BGR\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "        mask_path = os.path.join(self.image_dir, self.targets_name[idx])\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Keep 1-channel\n",
        "\n",
        "        mask = self.colormap_to_labelmap(mask)\n",
        "        image = Image.fromarray(image)\n",
        "        mask = Image.fromarray(mask.astype(np.uint8))\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def colormap_to_labelmap(self, mask):\n",
        "        label_values = np.array(list(self.label_map.values()))\n",
        "        mask = mask.reshape(-1, 3)\n",
        "        unique_colors, inverse = np.unique(mask, axis=0, return_inverse=True)\n",
        "        label_map = {tuple(color): idx for idx, color in enumerate(label_values)}\n",
        "\n",
        "        mapped_mask = np.array([label_map.get(tuple(color), 0) for color in unique_colors])\n",
        "        mapped_mask = mapped_mask[inverse].reshape(mask.shape[:2])\n",
        "        return mapped_mask.astype(np.int64)"
      ],
      "metadata": {
        "id": "9xh-eowuJji_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelV1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ðŸ”¹ Fixed Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        # Only apply shortcut when dimensions change\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_c != out_c:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_c)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)  # Shortcut path\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += identity  # Skip connection\n",
        "        return F.relu(x)\n",
        "\n",
        "\n",
        "# ðŸ”¹ Fixed ResNet Classifier\n",
        "class ModelV1(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ModelV1, self).__init__()\n",
        "\n",
        "        self.h1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.layer1 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer2 = ResidualBlock(128, 256, stride=2)\n",
        "        self.layer3 = ResidualBlock(256, 512, stride=2)\n",
        "        self.layer4 = ResidualBlock(512, 1024, stride=2)\n",
        "\n",
        "        self.pooled = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # final layer convert flatten() to classification of 7\n",
        "        self.fc = nn.Linear(1024, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.h1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.pooled(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "-Amd4FR3KmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pls copy and paster futher shit... from the OG files.."
      ],
      "metadata": {
        "id": "YX7_wQNDK26G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Train():\n",
        "    def __init__(self, model, dataloader, optimizer, num_epochs, device, loss):\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        self.loss = loss\n",
        "\n",
        "    def trainModel(self):\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_pixels = 0\n",
        "            total_pixels = 0\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            loop = tqdm(self.dataloader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
        "            for inputs, labels in loop:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                labels = labels.long()\n",
        "                self.model.train();\n",
        "                predictions = self.model(inputs)\n",
        "\n",
        "                if labels.dim() == 4:\n",
        "                    labels = labels.squeeze(1)\n",
        "\n",
        "                loss = self.loss(predictions, labels)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item() * inputs.size(0)  # Multiply by batch size\n",
        "\n",
        "                # Compute pixel accuracy\n",
        "                preds_flat = predictions.argmax(dim=1).cpu().numpy().flatten()\n",
        "                labels_flat = labels.cpu().numpy().flatten()\n",
        "\n",
        "                all_preds.extend(preds_flat)\n",
        "                all_labels.extend(labels_flat)\n",
        "\n",
        "                correct_pixels += (preds_flat == labels_flat).sum()\n",
        "                total_pixels += len(preds_flat)\n",
        "\n",
        "                # Update tqdm progress bar\n",
        "                loop.set_postfix({\n",
        "                    \"Loss\": total_loss / total_pixels,  # Normalized loss\n",
        "                    \"Accuracy\": correct_pixels / total_pixels\n",
        "                })\n",
        "\n",
        "            # Compute final epoch metrics\n",
        "            epoch_loss = total_loss / total_pixels  # Normalize loss\n",
        "            epoch_accuracy = correct_pixels / total_pixels\n",
        "            epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "            epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\n[Epoch {epoch + 1}/{self.num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "            print(f\"Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}, F1 Score: {epoch_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "w86yN7ibKrVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shit\n",
        "from torch.nn import CrossEntropyLoss\n",
        "num_classes = len(label_map.keys())\n",
        "lr = 0.001\n",
        "batch_size = 4\n",
        "train_dataset = Segmentation_Dataset(TRAIN_DIR, label_map, image_transform, mask_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "num_epochs = 2\n",
        "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loss = CrossEntropyLoss()\n",
        "\n",
        "\n",
        "model = ModelV1(n_class=num_classes).to(device)"
      ],
      "metadata": {
        "id": "QRcZixItL6Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Train(model, train_dataloader, optimizer, num_epochs, device, loss)\n",
        "trainer.trainModel()"
      ],
      "metadata": {
        "id": "ofa8Qr_RMUSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}